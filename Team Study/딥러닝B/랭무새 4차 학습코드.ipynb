{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ef808b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store already exists. No need to initialize.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Define the directory containing the text file and the persistent directory\n",
    "current_dir = os.getcwd()\n",
    "file_path = os.path.join(current_dir, \"books\", \"odyssey.txt\")\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db\")\n",
    "\n",
    "# Check if the Chroma vector store already exists\n",
    "if not os.path.exists(persistent_directory):\n",
    "    print(\"Persistent directory does not exist. Initializing vector store...\")\n",
    "\n",
    "    # Ensure the text file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"The file {file_path} does not exist. Please check the path.\"\n",
    "        )\n",
    "\n",
    "    # Read the text content from the file\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split the document into chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Display information about the split documents\n",
    "    print(\"\\n--- Document Chunks Information ---\")\n",
    "    print(f\"Number of document chunks: {len(docs)}\")\n",
    "    print(f\"Sample chunk:\\n{docs[0].page_content}\\n\")\n",
    "\n",
    "    # Create embeddings\n",
    "    print(\"\\n--- Creating embeddings ---\")\n",
    "    embeddings = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    # Update to a valid embedding model if needed\n",
    "    print(\"\\n--- Finished creating embeddings ---\")\n",
    "\n",
    "    # Create the vector store and persist it automatically\n",
    "    print(\"\\n--- Creating vector store ---\")\n",
    "    db = Chroma.from_documents(\n",
    "        docs, embeddings, persist_directory=persistent_directory)\n",
    "    print(\"\\n--- Finished creating vector store ---\")\n",
    "\n",
    "else:\n",
    "    print(\"Vector store already exists. No need to initialize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343985ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changeroa/envs/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No relevant docs were retrieved using the relevance score threshold 0.9\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Load the existing vector store with the embedding function\n",
    "db = Chroma(persist_directory=persistent_directory,\n",
    "            embedding_function=embedding_function)\n",
    "\n",
    "# Retriever 설정\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 3, \"score_threshold\": 0.9},\n",
    ")\n",
    "\n",
    "# 질의 실행\n",
    "query = \"What happened to Odysseus after Troy?\"\n",
    "relevant_docs = retriever.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc587166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books directory: /home/changeroa/projects/Agents/langchain-crash-course-main/4_rag/books\n",
      "Persistent directory: /home/changeroa/projects/Agents/langchain-crash-course-main/4_rag/db/chroma_db_with_metadata\n",
      "Vector store already exists. No need to initialize.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# 디렉토리 설정\n",
    "current_dir = os.getcwd()\n",
    "books_dir = os.path.join(current_dir, \"books\")\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "persistent_directory = os.path.join(db_dir, \"chroma_db_with_metadata\")\n",
    "\n",
    "print(f\"Books directory: {books_dir}\")\n",
    "print(f\"Persistent directory: {persistent_directory}\")\n",
    "\n",
    "# 벡터 스토어 초기화 여부 확인\n",
    "if not os.path.exists(persistent_directory):\n",
    "    print(\"Persistent directory does not exist. Initializing vector store...\")\n",
    "\n",
    "    if not os.path.exists(books_dir):\n",
    "        raise FileNotFoundError(\n",
    "            f\"The directory {books_dir} does not exist. Please check the path.\"\n",
    "        )\n",
    "\n",
    "    # .txt 파일 목록\n",
    "    book_files = [f for f in os.listdir(books_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "    # 문서 로드 및 메타데이터 추가\n",
    "    documents = []\n",
    "    for book_file in book_files:\n",
    "        file_path = os.path.join(books_dir, book_file)\n",
    "        loader = TextLoader(file_path)\n",
    "        book_docs = loader.load()\n",
    "        for doc in book_docs:\n",
    "            doc.metadata = {\"source\": book_file}\n",
    "            documents.append(doc)\n",
    "\n",
    "    # 문서 분할\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    print(\"\\n--- Document Chunks Information ---\")\n",
    "    print(f\"Number of document chunks: {len(docs)}\")\n",
    "\n",
    "    # HuggingFace 임베딩 생성\n",
    "    print(\"\\n--- Creating embeddings ---\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "    print(\"\\n--- Finished creating embeddings ---\")\n",
    "\n",
    "    # 벡터 스토어 생성 및 저장\n",
    "    print(\"\\n--- Creating and persisting vector store ---\")\n",
    "    db = Chroma.from_documents(\n",
    "        docs, embeddings, persist_directory=persistent_directory\n",
    "    )\n",
    "    print(\"\\n--- Finished creating and persisting vector store ---\")\n",
    "\n",
    "else:\n",
    "    print(\"Vector store already exists. No need to initialize.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02941dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relevant Documents ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Define the persistent directory\n",
    "current_dir = os.getcwd()\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "persistent_directory = os.path.join(db_dir, \"chroma_db_with_metadata\")\n",
    "\n",
    "# Define the embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Load the existing vector store with the embedding function\n",
    "db = Chroma(persist_directory=persistent_directory,\n",
    "            embedding_function=embeddings)\n",
    "\n",
    "# Define the user's question\n",
    "query = \"How did Juliet die?\"\n",
    "\n",
    "# Retrieve relevant documents based on the query\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 3, \"score_threshold\": 0.1},\n",
    ")\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "# Display the relevant results with metadata\n",
    "print(\"\\n--- Relevant Documents ---\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a8caf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1105, which is longer than the specified 1000\n",
      "Created a chunk of size 1437, which is longer than the specified 1000\n",
      "Created a chunk of size 1871, which is longer than the specified 1000\n",
      "Created a chunk of size 1015, which is longer than the specified 1000\n",
      "Created a chunk of size 1006, which is longer than the specified 1000\n",
      "Created a chunk of size 1054, which is longer than the specified 1000\n",
      "Created a chunk of size 1432, which is longer than the specified 1000\n",
      "Created a chunk of size 1367, which is longer than the specified 1000\n",
      "Created a chunk of size 2178, which is longer than the specified 1000\n",
      "Created a chunk of size 1390, which is longer than the specified 1000\n",
      "Created a chunk of size 1502, which is longer than the specified 1000\n",
      "Created a chunk of size 1410, which is longer than the specified 1000\n",
      "Created a chunk of size 1741, which is longer than the specified 1000\n",
      "Created a chunk of size 1184, which is longer than the specified 1000\n",
      "Created a chunk of size 1045, which is longer than the specified 1000\n",
      "Created a chunk of size 1132, which is longer than the specified 1000\n",
      "Created a chunk of size 1674, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using Character-based Splitting ---\n",
      "\n",
      "--- Creating vector store chroma_db_char ---\n",
      "--- Finished creating vector store chroma_db_char ---\n",
      "\n",
      "--- Using Sentence-based Splitting ---\n",
      "\n",
      "--- Creating vector store chroma_db_sent ---\n",
      "--- Finished creating vector store chroma_db_sent ---\n",
      "\n",
      "--- Using Token-based Splitting ---\n",
      "\n",
      "--- Creating vector store chroma_db_token ---\n",
      "--- Finished creating vector store chroma_db_token ---\n",
      "\n",
      "--- Using Recursive Character-based Splitting ---\n",
      "\n",
      "--- Creating vector store chroma_db_rec_char ---\n",
      "--- Finished creating vector store chroma_db_rec_char ---\n",
      "\n",
      "--- Using Custom Splitting ---\n",
      "\n",
      "--- Creating vector store chroma_db_custom ---\n",
      "--- Finished creating vector store chroma_db_custom ---\n",
      "\n",
      "--- Querying the Vector Store chroma_db_char ---\n",
      "\n",
      "--- Relevant Documents for chroma_db_char ---\n",
      "Document 1:\n",
      "PARIS.\n",
      "O, I am slain! [_Falls._] If thou be merciful,\n",
      "Open the tomb, lay me with Juliet.\n",
      "\n",
      " [_Dies._]\n",
      "\n",
      "ROMEO.\n",
      "In faith, I will. Let me peruse this face.\n",
      "Mercutio’s kinsman, noble County Paris!\n",
      "What said my man, when my betossed soul\n",
      "Did not attend him as we rode? I think\n",
      "He told me Paris should have married Juliet.\n",
      "Said he not so? Or did I dream it so?\n",
      "Or am I mad, hearing him talk of Juliet,\n",
      "To think it was so? O, give me thy hand,\n",
      "One writ with me in sour misfortune’s book.\n",
      "I’ll bury thee in a triumphant grave.\n",
      "A grave? O no, a lantern, slaught’red youth,\n",
      "For here lies Juliet, and her beauty makes\n",
      "This vault a feasting presence full of light.\n",
      "Death, lie thou there, by a dead man interr’d.\n",
      "\n",
      " [_Laying Paris in the monument._]\n",
      "\n",
      "Source: /home/changeroa/projects/Agents/langchain-crash-course-main/4_rag/books/romeo_and_juliet.txt\n",
      "\n",
      "\n",
      "--- Querying the Vector Store chroma_db_sent ---\n",
      "\n",
      "--- Relevant Documents for chroma_db_sent ---\n",
      "Document 1:\n",
      "i am the greatest, able to do least, yet most suspected, as the time and place doth make against me, of this direful murder. and here i stand, both to impeach and purge myself condemned and myself excus ’ d. prince. then say at once what thou dost know in this. friar lawrence. i will be brief, for my short date of breath is not so long as is a tedious tale. romeo, there dead, was husband to that juliet, and she, there dead, that romeo ’ s faithful wife. i married them ; and their stol ’ n marriage day was tybalt ’ s doomsday, whose untimely death banish ’ d the new - made bridegroom from this city ; for whom, and not for tybalt, juliet pin ’ d. you, to remove that siege of grief from her, betroth ’ d, and would have married her perforce to county paris. then comes she to me, and with wild looks, bid me devise some means to rid her from this second marriage, or in my cell there would she kill herself. then gave i her, so tutored by my art, a sleeping potion, which so took effect as i intended, for it wrought on her the form of death. meantime i writ to romeo that he should hither come as this dire night to help to take her from her borrow ’ d grave, being the time the potion ’ s force should cease. but he which bore my letter, friar john, was stay ’ d by accident ; and yesternight return ’ d my letter back. then all alone at the prefixed hour of her waking came i to take her from her kindred ’ s vault, meaning to keep her closely at my cell till i conveniently could send to romeo. but when i came, some minute er\n",
      "\n",
      "Source: /home/changeroa/projects/Agents/langchain-crash-course-main/4_rag/books/romeo_and_juliet.txt\n",
      "\n",
      "\n",
      "--- Querying the Vector Store chroma_db_token ---\n",
      "\n",
      "--- Relevant Documents for chroma_db_token ---\n",
      "Document 1:\n",
      " from this second marriage,\n",
      "Or in my cell there would she kill herself.\n",
      "Then gave I her, so tutored by my art,\n",
      "A sleeping potion, which so took effect\n",
      "As I intended, for it wrought on her\n",
      "The form of death. Meantime I writ to Romeo\n",
      "That he should hither come as this dire night\n",
      "To help to take her from her borrow’d grave,\n",
      "Being the time the potion’s force should cease.\n",
      "But he which bore my letter, Friar John,\n",
      "Was stay’d by accident; and yesternight\n",
      "Return’d my letter back. Then all alone\n",
      "At the prefixed hour of her waking\n",
      "Came I to take her from her kindred’s vault,\n",
      "Meaning to keep her closely at my cell\n",
      "Till I conveniently could send to Romeo.\n",
      "But when I came, some minute ere the time\n",
      "Of her awaking, here untimely lay\n",
      "The noble Paris and true Romeo dead.\n",
      "She wakes; and I entreated her come forth\n",
      "And bear this work of heaven with patience.\n",
      "But then a noise did scare me from the tomb;\n",
      "And she, too desperate, would not go with me,\n",
      "But, as it seems, did violence on herself.\n",
      "All this I know; and to the marriage\n",
      "Her Nurse is privy. And if ought in this\n",
      "Miscarried by my fault, let my old life\n",
      "Be sacrific’d, some hour before his time,\n",
      "Unto the rigour of severest law.\n",
      "\n",
      "PRINCE.\n",
      "We still have known thee for a holy man.\n",
      "Where’s Romeo’s man? What can he say to this?\n",
      "\n",
      "BALTHASAR.\n",
      "I brought my master news of Juliet’s death,\n",
      "And then in post he came from Mantua\n",
      "To this same place, to this same monument.\n",
      "This letter he early bid me give his father,\n",
      "And threaten’d me with death, going in the vault,\n",
      "If I departed not, and left him there.\n",
      "\n",
      "PRINCE.\n",
      "Give me the letter, I will look on it.\n",
      "Where is the County’s Page that rais’d the watch?\n",
      "Sirrah, what made your master in this place?\n",
      "\n",
      "PAGE.\n",
      "He came with flowers to strew his lady’s grave,\n",
      "And\n",
      "\n",
      "Source: /home/changeroa/projects/Agents/langchain-crash-course-main/4_rag/books/romeo_and_juliet.txt\n",
      "\n",
      "\n",
      "--- Querying the Vector Store chroma_db_rec_char ---\n",
      "\n",
      "--- Relevant Documents for chroma_db_rec_char ---\n",
      "Document 1:\n",
      "JULIET.\n",
      "Blister’d be thy tongue\n",
      "For such a wish! He was not born to shame.\n",
      "Upon his brow shame is asham’d to sit;\n",
      "For ’tis a throne where honour may be crown’d\n",
      "Sole monarch of the universal earth.\n",
      "O, what a beast was I to chide at him!\n",
      "\n",
      "NURSE.\n",
      "Will you speak well of him that kill’d your cousin?\n",
      "\n",
      "Source: /home/changeroa/projects/Agents/langchain-crash-course-main/4_rag/books/romeo_and_juliet.txt\n",
      "\n",
      "\n",
      "--- Querying the Vector Store chroma_db_custom ---\n",
      "\n",
      "--- Relevant Documents for chroma_db_custom ---\n",
      "Document 1:\n",
      "LADY CAPULET.\n",
      "Why, how now, Juliet?\n",
      "\n",
      "Source: /home/changeroa/projects/Agents/langchain-crash-course-main/4_rag/books/romeo_and_juliet.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTokenTextSplitter,\n",
    "    TextSplitter,\n",
    "    TokenTextSplitter,\n",
    ")\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# 디렉토리 설정\n",
    "current_dir = os.getcwd()\n",
    "file_path = os.path.join(current_dir, \"books\", \"romeo_and_juliet.txt\")\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "\n",
    "# 텍스트 파일 존재 여부 확인\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"The file {file_path} does not exist. Please check the path.\")\n",
    "\n",
    "# 문서 로드\n",
    "loader = TextLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# 임베딩 모델 정의\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# 벡터스토어 생성 함수\n",
    "def create_vector_store(docs, store_name):\n",
    "    persistent_directory = os.path.join(db_dir, store_name)\n",
    "    if not os.path.exists(persistent_directory):\n",
    "        print(f\"\\n--- Creating vector store {store_name} ---\")\n",
    "        db = Chroma.from_documents(\n",
    "            docs, embeddings, persist_directory=persistent_directory\n",
    "        )\n",
    "        print(f\"--- Finished creating vector store {store_name} ---\")\n",
    "    else:\n",
    "        print(f\"Vector store {store_name} already exists. No need to initialize.\")\n",
    "\n",
    "# 1. Character-based Splitting\n",
    "print(\"\\n--- Using Character-based Splitting ---\")\n",
    "char_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "char_docs = char_splitter.split_documents(documents)\n",
    "create_vector_store(char_docs, \"chroma_db_char\")\n",
    "\n",
    "# 2. Sentence-based Splitting\n",
    "print(\"\\n--- Using Sentence-based Splitting ---\")\n",
    "sent_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1000)\n",
    "sent_docs = sent_splitter.split_documents(documents)\n",
    "create_vector_store(sent_docs, \"chroma_db_sent\")\n",
    "\n",
    "# 3. Token-based Splitting\n",
    "print(\"\\n--- Using Token-based Splitting ---\")\n",
    "token_splitter = TokenTextSplitter(chunk_overlap=0, chunk_size=512)\n",
    "token_docs = token_splitter.split_documents(documents)\n",
    "create_vector_store(token_docs, \"chroma_db_token\")\n",
    "\n",
    "# 4. Recursive Character-based Splitting\n",
    "print(\"\\n--- Using Recursive Character-based Splitting ---\")\n",
    "rec_char_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "rec_char_docs = rec_char_splitter.split_documents(documents)\n",
    "create_vector_store(rec_char_docs, \"chroma_db_rec_char\")\n",
    "\n",
    "# 5. Custom Splitting\n",
    "print(\"\\n--- Using Custom Splitting ---\")\n",
    "class CustomTextSplitter(TextSplitter):\n",
    "    def split_text(self, text):\n",
    "        return text.split(\"\\n\\n\")  # 단락 기준\n",
    "\n",
    "custom_splitter = CustomTextSplitter()\n",
    "custom_docs = custom_splitter.split_documents(documents)\n",
    "create_vector_store(custom_docs, \"chroma_db_custom\")\n",
    "\n",
    "# 질의 함수 정의\n",
    "def query_vector_store(store_name, query):\n",
    "    persistent_directory = os.path.join(db_dir, store_name)\n",
    "    if os.path.exists(persistent_directory):\n",
    "        print(f\"\\n--- Querying the Vector Store {store_name} ---\")\n",
    "        db = Chroma(\n",
    "            persist_directory=persistent_directory, embedding_function=embeddings\n",
    "        )\n",
    "        retriever = db.as_retriever(\n",
    "            search_type=\"similarity_score_threshold\",\n",
    "            search_kwargs={\"k\": 1, \"score_threshold\": 0.1},\n",
    "        )\n",
    "        relevant_docs = retriever.invoke(query)\n",
    "        print(f\"\\n--- Relevant Documents for {store_name} ---\")\n",
    "        for i, doc in enumerate(relevant_docs, 1):\n",
    "            print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "            print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n",
    "    else:\n",
    "        print(f\"Vector store {store_name} does not exist.\")\n",
    "\n",
    "# 사용자 질의 정의\n",
    "query = \"How did Juliet die?\"\n",
    "\n",
    "# 각 벡터스토어에 질의 실행\n",
    "query_vector_store(\"chroma_db_char\", query)\n",
    "query_vector_store(\"chroma_db_sent\", query)\n",
    "query_vector_store(\"chroma_db_token\", query)\n",
    "query_vector_store(\"chroma_db_rec_char\", query)\n",
    "query_vector_store(\"chroma_db_custom\", query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "281614c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "Created a chunk of size 1070, which is longer than the specified 1000\n",
      "Created a chunk of size 1305, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document Chunks Information ---\n",
      "Number of document chunks: 7\n",
      "Sample chunk:\n",
      "Apple\n",
      "\n",
      "\n",
      "Apple\n",
      "\n",
      "AppleStoreMaciPadiPhoneWatch\n",
      "VisionAirPodsTV & HomeEntertainmentAccessoriesSupport\n",
      "\n",
      "\n",
      "0+\n",
      "\n",
      " \n",
      "\n",
      "iPhone 16 Pro\n",
      "Hello, Apple Intelligence.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "iPhone 16\n",
      "Hello, Apple Intelligence.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Apple Watch Series 10\n",
      "Thinstant classic.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Apple Watch Ultra 2\n",
      "New finish. Never quit.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Apple Watch\n",
      "Live healthier. Train better. Stay connected.\n",
      "\n",
      "Learn more\n",
      "Shop Watch\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "WWDC 25\n",
      "\n",
      "Apple Worldwide Developers Conference. Join us online June 9–13.\n",
      "\n",
      "Learn more\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "MacBook Air\n",
      "Sky blue color.Sky high performance with M4.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "Built for Apple Intelligence.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "iPad Air\n",
      "Now supercharged by the M3 chip.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "Built for Apple Intelligence.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "AirPods Pro 2\n",
      "Now with a Hearing Aid feature.1\n",
      "\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Apple Trade In\n",
      "Get $170–$630 in credit when you trade in iPhone 12 or higher.2\n",
      "\n",
      "Get your estimate\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Apple Card\n",
      "Get up to 3% Daily Cash back with every purchase.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changeroa/envs/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating vector store in /home/changeroa/projects/Agents/langchain-crash-course-main/4_rag/db/chroma_db_apple ---\n",
      "--- Finished creating vector store in /home/changeroa/projects/Agents/langchain-crash-course-main/4_rag/db/chroma_db_apple ---\n",
      "\n",
      "--- Relevant Documents ---\n",
      "Document 1:\n",
      "Apple\n",
      "\n",
      "\n",
      "Apple\n",
      "\n",
      "AppleStoreMaciPadiPhoneWatch\n",
      "VisionAirPodsTV & HomeEntertainmentAccessoriesSupport\n",
      "\n",
      "\n",
      "0+\n",
      "\n",
      " \n",
      "\n",
      "iPhone 16 Pro\n",
      "Hello, Apple Intelligence.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "iPhone 16\n",
      "Hello, Apple Intelligence.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Apple Watch Series 10\n",
      "Thinstant classic.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Apple Watch Ultra 2\n",
      "New finish. Never quit.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Apple Watch\n",
      "Live healthier. Train better. Stay connected.\n",
      "\n",
      "Learn more\n",
      "Shop Watch\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "WWDC 25\n",
      "\n",
      "Apple Worldwide Developers Conference. Join us online June 9–13.\n",
      "\n",
      "Learn more\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "MacBook Air\n",
      "Sky blue color.Sky high performance with M4.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "Built for Apple Intelligence.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "iPad Air\n",
      "Now supercharged by the M3 chip.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "Built for Apple Intelligence.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "AirPods Pro 2\n",
      "Now with a Hearing Aid feature.1\n",
      "\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Apple Trade In\n",
      "Get $170–$630 in credit when you trade in iPhone 12 or higher.2\n",
      "\n",
      "Get your estimate\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Apple Card\n",
      "Get up to 3% Daily Cash back with every purchase.\n",
      "\n",
      "Source: https://www.apple.com/\n",
      "\n",
      "Document 2:\n",
      "A subscription is required for Apple Arcade, Apple Fitness+, Apple Music, and Apple TV+.\n",
      "\n",
      "Features are subject to change. Some features, applications, and services may not be available in all regions or all languages.\n",
      "\n",
      "Shop and Learn\n",
      "\n",
      "Shop and Learn\n",
      "\n",
      "\n",
      "Store\n",
      "Mac\n",
      "iPad\n",
      "iPhone\n",
      "Watch\n",
      "Vision\n",
      "AirPods\n",
      "TV & Home\n",
      "AirTag\n",
      "Accessories\n",
      "Gift Cards\n",
      "\n",
      "\n",
      "Apple Wallet\n",
      "\n",
      "Apple Wallet\n",
      "\n",
      "\n",
      "Wallet\n",
      "Apple Card\n",
      "Apple Pay\n",
      "Apple Cash\n",
      "\n",
      "\n",
      "Account\n",
      "\n",
      "Account\n",
      "\n",
      "\n",
      "Manage Your Apple Account\n",
      "Apple Store Account\n",
      "iCloud.com\n",
      "\n",
      "\n",
      "Entertainment\n",
      "\n",
      "Entertainment\n",
      "\n",
      "\n",
      "Apple One\n",
      "Apple TV+\n",
      "Apple Music\n",
      "Apple Arcade\n",
      "Apple Fitness+\n",
      "Apple News+\n",
      "Apple Podcasts\n",
      "Apple Books\n",
      "App Store\n",
      "\n",
      "\n",
      "Apple Store\n",
      "\n",
      "Apple Store\n",
      "\n",
      "\n",
      "Find a Store\n",
      "Genius Bar\n",
      "Today at Apple\n",
      "Group Reservations\n",
      "Apple Camp\n",
      "Apple Store App\n",
      "Certified Refurbished\n",
      "Apple Trade In\n",
      "Financing\n",
      "Carrier Deals at Apple\n",
      "Order Status\n",
      "Shopping Help\n",
      "\n",
      "\n",
      "For Business\n",
      "\n",
      "For Business\n",
      "\n",
      "\n",
      "Apple and Business\n",
      "Shop for Business\n",
      "\n",
      "\n",
      "For Education\n",
      "\n",
      "For Education\n",
      "\n",
      "\n",
      "Apple and Education\n",
      "Shop for K-12\n",
      "Shop for College\n",
      "\n",
      "Source: https://www.apple.com/\n",
      "\n",
      "Document 3:\n",
      "For Healthcare\n",
      "\n",
      "For Healthcare\n",
      "\n",
      "\n",
      "Apple in Healthcare\n",
      "Mac in Healthcare\n",
      "Health on Apple Watch\n",
      "Health Records on iPhone and iPad\n",
      "\n",
      "\n",
      "For Government\n",
      "\n",
      "For Government\n",
      "\n",
      "\n",
      "Shop for Government\n",
      "Shop for Veterans and Military\n",
      "\n",
      "\n",
      "Apple Values\n",
      "\n",
      "Apple Values\n",
      "\n",
      "\n",
      "Accessibility\n",
      "Education\n",
      "Environment\n",
      "Inclusion and Diversity\n",
      "Privacy\n",
      "Racial Equity and Justice\n",
      "Supply Chain\n",
      "\n",
      "\n",
      "About Apple\n",
      "\n",
      "About Apple\n",
      "\n",
      "\n",
      "Newsroom\n",
      "Apple Leadership\n",
      "Career Opportunities\n",
      "Investors\n",
      "Ethics & Compliance\n",
      "Events\n",
      "Contact Apple\n",
      "\n",
      "\n",
      "\t\t\tMore ways to shop: Find an Apple Store or other retailer near you. Or call 1-800-MY-APPLE.\n",
      "\n",
      "\n",
      "United States\n",
      "\n",
      "\n",
      "Copyright ©\n",
      "\t\t\t\t\n",
      "\t\t\t\t2025\n",
      "\t\t\t\t Apple Inc. All rights reserved.\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "Privacy Policy\n",
      "\n",
      "\n",
      "Terms of Use\n",
      "\n",
      "\n",
      "Sales and Refunds\n",
      "\n",
      "\n",
      "Legal\n",
      "\n",
      "\n",
      "Site Map\n",
      "\n",
      "Source: https://www.apple.com/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# Define the persistent directory\n",
    "current_dir = os.getcwd()  # Jupyter 환경 호환\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "persistent_directory = os.path.join(db_dir, \"chroma_db_apple\")\n",
    "\n",
    "# Step 1: Scrape content from apple.com\n",
    "urls = [\"https://www.apple.com/\"]\n",
    "loader = WebBaseLoader(urls)\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Split the scraped content into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(\"\\n--- Document Chunks Information ---\")\n",
    "print(f\"Number of document chunks: {len(docs)}\")\n",
    "print(f\"Sample chunk:\\n{docs[0].page_content}\\n\")\n",
    "\n",
    "# Step 3: Use HuggingFace embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Step 4: Create or load the vector store\n",
    "if not os.path.exists(persistent_directory):\n",
    "    print(f\"\\n--- Creating vector store in {persistent_directory} ---\")\n",
    "    db = Chroma.from_documents(docs, embeddings, persist_directory=persistent_directory)\n",
    "    print(f\"--- Finished creating vector store in {persistent_directory} ---\")\n",
    "else:\n",
    "    print(f\"Vector store {persistent_directory} already exists. Loading existing store.\")\n",
    "    db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)\n",
    "\n",
    "# Step 5: Query the vector store\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "query = \"What new products are announced on Apple.com?\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n--- Relevant Documents ---\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "    if doc.metadata:\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
