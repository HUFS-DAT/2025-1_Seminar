{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d549f33-f955-47cf-8599-62f0f363be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from SwiFT.project.module.models.swin4d_transformer_ver7 import SwinTransformer4D\n",
    "from nilearn.image import load_img, resample_img\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "# 사용자 설정\n",
    "data_root = \"./data\"\n",
    "save_root = \"./fmri_pooled_features\"\n",
    "metadata_csv = \"fmri_volumes_with_events.csv\"\n",
    "TR = 1.5\n",
    "WINDOW_SIZE = 20\n",
    "target_shape = (96, 96, 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad39d6ed-6db6-449d-9036-01fc0ccef874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_size:  (96, 96, 96, 20)\n",
      "patch_size:  (6, 6, 6, 1)\n",
      "patch_dim:  (16, 16, 16, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinTransformer4D(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (fc): Linear(in_features=216, out_features=36, bias=True)\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (pos_embeds): ModuleList(\n",
       "    (0-3): 4 x PositionalEmbedding()\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x SwinTransformerBlock4D(\n",
       "          (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention4D(\n",
       "            (qkv): Linear(in_features=36, out_features=108, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=36, out_features=36, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (linear1): Linear(in_features=36, out_features=144, bias=True)\n",
       "            (linear2): Linear(in_features=144, out_features=36, bias=True)\n",
       "            (fn): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMergingV2(\n",
       "        (reduction): Linear(in_features=288, out_features=72, bias=False)\n",
       "        (norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x SwinTransformerBlock4D(\n",
       "          (norm1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention4D(\n",
       "            (qkv): Linear(in_features=72, out_features=216, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=72, out_features=72, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (linear1): Linear(in_features=72, out_features=288, bias=True)\n",
       "            (linear2): Linear(in_features=288, out_features=72, bias=True)\n",
       "            (fn): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMergingV2(\n",
       "        (reduction): Linear(in_features=576, out_features=144, bias=False)\n",
       "        (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0-5): 6 x SwinTransformerBlock4D(\n",
       "          (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention4D(\n",
       "            (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (linear1): Linear(in_features=144, out_features=576, bias=True)\n",
       "            (linear2): Linear(in_features=576, out_features=144, bias=True)\n",
       "            (fn): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (downsample): PatchMergingV2(\n",
       "        (reduction): Linear(in_features=1152, out_features=288, bias=False)\n",
       "        (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x SwinTransformerBlock4D(\n",
       "          (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention4D(\n",
       "            (qkv): Linear(in_features=288, out_features=864, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=288, out_features=288, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (linear1): Linear(in_features=288, out_features=1152, bias=True)\n",
       "            (linear2): Linear(in_features=1152, out_features=288, bias=True)\n",
       "            (fn): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 (사용자 정의 encoder 준비)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ckpt_path = \"SwiFT/pretrained_models/contrastive_pretrained.ckpt\"\n",
    "#ckpt_path = \"SwiFT/pretrained_models/hcp_sex_classification.ckpt\"\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "# state_dict만 추출 (PyTorch Lightning 형식일 경우 'state_dict' 키 있음)\n",
    "if \"state_dict\" in ckpt:\n",
    "    state_dict = {k.replace(\"model.\", \"\"): v for k, v in ckpt[\"state_dict\"].items()}\n",
    "else:\n",
    "    state_dict = ckpt\n",
    "\n",
    "model = SwinTransformer4D(\n",
    "    img_size=(96, 96, 96, 20),\n",
    "    in_chans=1,\n",
    "    embed_dim=36,\n",
    "    window_size=(4, 4, 4, 4),\n",
    "    first_window_size=(2, 2, 2, 2),\n",
    "    patch_size=(6, 6, 6, 1),\n",
    "    depths=(2, 2, 6, 2),\n",
    "    num_heads=(3, 6, 12, 24),\n",
    "    downsample=\"mergingv2\",\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d42c8d-3339-40d3-8313-ee63141237bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 288])\n"
     ]
    }
   ],
   "source": [
    "print(model(torch.randn(1, 1, 96, 96, 96, 20).to(device)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd8b2cee-2b72-4fab-976c-5517a1cbcca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['head.weight', 'head.bias', 'emb_mlp.fc1.weight', 'emb_mlp.bn1.weight', 'emb_mlp.bn1.bias', 'emb_mlp.bn1.running_mean', 'emb_mlp.bn1.running_var', 'emb_mlp.bn1.num_batches_tracked'])\n"
     ]
    }
   ],
   "source": [
    "res = model.load_state_dict(state_dict, strict=False)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae73c023-62f0-4d61-b671-9654fbd431ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메타데이터 불러오기\n",
    "df = pd.read_csv(metadata_csv)\n",
    "df = df.iloc[630:].reset_index(drop=True)\n",
    "\n",
    "df = df[df['trial_type'].str.contains(\"story\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ab011f-2b31-4495-8bbf-1ff7a9a28b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>file</th>\n",
       "      <th>task</th>\n",
       "      <th>n_volumes</th>\n",
       "      <th>TR_nii</th>\n",
       "      <th>TR_json</th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>onset</th>\n",
       "      <th>duration</th>\n",
       "      <th>trial_type</th>\n",
       "      <th>stim_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-264</td>\n",
       "      <td>sub-264_task-21styear_bold.nii.gz</td>\n",
       "      <td>21styear</td>\n",
       "      <td>2249</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3373.5</td>\n",
       "      <td>0.0,21.0</td>\n",
       "      <td>18.0,3338.0</td>\n",
       "      <td>music,story</td>\n",
       "      <td>21styear_audio.wav,21styear_audio.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-265</td>\n",
       "      <td>sub-265_task-piemanpni_bold.nii.gz</td>\n",
       "      <td>piemanpni</td>\n",
       "      <td>294</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>441.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>story</td>\n",
       "      <td>piemanpni_audio.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-265</td>\n",
       "      <td>sub-265_task-forgot_bold.nii.gz</td>\n",
       "      <td>forgot</td>\n",
       "      <td>574</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>861.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>837.0</td>\n",
       "      <td>story</td>\n",
       "      <td>forgot_audio.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-265</td>\n",
       "      <td>sub-265_task-21styear_bold.nii.gz</td>\n",
       "      <td>21styear</td>\n",
       "      <td>2249</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3373.5</td>\n",
       "      <td>0.0,21.0</td>\n",
       "      <td>18.0,3338.0</td>\n",
       "      <td>music,story</td>\n",
       "      <td>21styear_audio.wav,21styear_audio.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-265</td>\n",
       "      <td>sub-265_task-bronx_bold.nii.gz</td>\n",
       "      <td>bronx</td>\n",
       "      <td>390</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>585.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>story</td>\n",
       "      <td>bronx_audio.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject                                file       task  n_volumes  TR_nii  \\\n",
       "0  sub-264   sub-264_task-21styear_bold.nii.gz   21styear       2249     1.5   \n",
       "1  sub-265  sub-265_task-piemanpni_bold.nii.gz  piemanpni        294     1.5   \n",
       "2  sub-265     sub-265_task-forgot_bold.nii.gz     forgot        574     1.5   \n",
       "3  sub-265   sub-265_task-21styear_bold.nii.gz   21styear       2249     1.5   \n",
       "4  sub-265      sub-265_task-bronx_bold.nii.gz      bronx        390     1.5   \n",
       "\n",
       "   TR_json  duration_sec     onset     duration   trial_type  \\\n",
       "0      1.5        3373.5  0.0,21.0  18.0,3338.0  music,story   \n",
       "1      1.5         441.0      12.0        400.0        story   \n",
       "2      1.5         861.0      12.0        837.0        story   \n",
       "3      1.5        3373.5  0.0,21.0  18.0,3338.0  music,story   \n",
       "4      1.5         585.0      12.0        536.0        story   \n",
       "\n",
       "                               stim_file  \n",
       "0  21styear_audio.wav,21styear_audio.wav  \n",
       "1                    piemanpni_audio.wav  \n",
       "2                       forgot_audio.wav  \n",
       "3  21styear_audio.wav,21styear_audio.wav  \n",
       "4                        bronx_audio.wav  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f013f8cc-4436-4e99-831b-9e5ca6d28f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 243/243 [46:26<00:00, 11.47s/it]\n"
     ]
    }
   ],
   "source": [
    "def extract_story_onset_duration(row):\n",
    "    types = row['trial_type'].split(',')\n",
    "    onsets = list(map(float, row['onset'].split(',')))\n",
    "    durations = list(map(float, row['duration'].split(',')))\n",
    "    story_idx = [i for i, t in enumerate(types) if t.strip() == \"story\"]\n",
    "    return [onsets[i] for i in story_idx], [durations[i] for i in story_idx]\n",
    "\n",
    "def resample_to_target(func_path, anat_path, target_shape=(96, 96, 96)):\n",
    "    anat_img = load_img(anat_path)\n",
    "    func_img = load_img(func_path)\n",
    "    zoom_factors = np.array(anat_img.shape) / np.array(target_shape)\n",
    "    target_affine = anat_img.affine.copy()\n",
    "    target_affine[:3, :3] = anat_img.affine[:3, :3] @ np.diag(zoom_factors)\n",
    "    resampled = resample_img(func_img, target_affine=target_affine,\n",
    "                             target_shape=target_shape, interpolation=\"nearest\")\n",
    "    return resampled.get_fdata()\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    subject = row['subject']\n",
    "    task = row['task']\n",
    "    file_name = row['file']\n",
    "    story_onsets, story_durations = extract_story_onset_duration(row)\n",
    "\n",
    "    func_path = os.path.join(data_root, subject, \"func\", file_name)\n",
    "    scan_tsv = os.path.join(data_root, subject, f\"{subject}_scans.tsv\")\n",
    "    \n",
    "    if not os.path.exists(func_path) or not os.path.exists(scan_tsv):\n",
    "        print(f\"[!] Missing file for {subject}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        scan_df = pd.read_csv(scan_tsv, sep=\"\\t\")\n",
    "        anat_row = scan_df[scan_df[\"filename\"].str.startswith(\"anat/\")].iloc[0]\n",
    "        anat_path = os.path.join(data_root, subject, anat_row[\"filename\"])\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Failed to find anat for {subject}: {e}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        resampled_data = resample_to_target(func_path, anat_path, target_shape)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Failed to resample {func_path}: {e}\")\n",
    "        continue\n",
    "    stride = 20\n",
    "\n",
    "    for onset, duration in zip(story_onsets, story_durations):\n",
    "        start_vol = int(onset // TR)\n",
    "        end_vol = int((onset + duration) // TR)\n",
    "\n",
    "        for i in range(start_vol, end_vol - WINDOW_SIZE + 1, stride):\n",
    "            vol = resampled_data[..., i:i + WINDOW_SIZE]\n",
    "            vol_tensor = torch.tensor(vol).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pooled = model(vol_tensor)  # (1, 96)\n",
    "                pooled = pooled.squeeze(0).cpu()\n",
    "\n",
    "            run_name = file_name.replace(\".nii.gz\", \"\")\n",
    "            save_path = os.path.join(save_root, subject, task, run_name, f\"frame_{i}.pt\")\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            torch.save(pooled, save_path)\n",
    "\n",
    "    del resampled_data, pooled, vol, vol_tensor\n",
    "    resampled_data = None\n",
    "    pooled = None\n",
    "    \n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c04322-6f4b-4e31-b2e9-6d521af4efd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swift",
   "language": "python",
   "name": "swift"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
